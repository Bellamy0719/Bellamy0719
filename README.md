# 👋 Hi, I'm Hongming Liu

**Data Engineer | Data Analyst | Data Scientist in Progress | Cloud & AI Enthusiast**

With 3 years of hands-on experience in data engineering and analytics, I build scalable cloud data pipelines and intelligent analytics systems that connect engineering precision with data insight.

---

## 🧩 Technical Focus

**Core Expertise**  
ETL Automation · PySpark · Airflow · Docker · SQL · Data Modeling · AWS · Databricks · Streaming (Kinesis)  

**Expanding Skills**  
dbt · MLflow · SageMaker · Feature Store · Data Governance (Delta Lake / Iceberg)  

**Analytical & Data Science Background**  
Experienced in classical modeling: Linear Regression, Logistic Regression, Decision Trees, and Time Series Analysis.  
Currently exploring Machine Learning and AI applications — including model deployment, feature pipelines, and Transformer-based architectures.

---

## 🚀 Project Highlights

### Airflow + Docker + PostgreSQL + Tableau
Automated batch ETL pipeline for stock data — covering ingestion, feature engineering, and dashboard visualization.  
**Focus:** Workflow orchestration, data validation, automation.

➡️ [View Project](https://github.com/Bellamy0719/airflow-stock-pipeline)

---

### AWS + Databricks + PySpark Lakehouse
Designed a 3-tier S3 data lake (raw, processed, curated) using PySpark, Glue, Athena, and QuickSight.  
**Focus:** Scalable analytics, data lakehouse architecture, schema management.

➡️ [View Project](https://github.com/Bellamy0719/aws-pyspark-data-lakehouse-pipeline)

---

### Kinesis + Databricks Streaming + S3
Real-time streaming pipeline with PySpark Structured Streaming.  
**Focus:** Windowing, watermarking, incremental writes, and near real-time aggregation.

➡️ [View Project](https://github.com/yourusername/aws-kinesis-streaming-pipeline)

---

## 📈 Data Science & AI Direction

Building on my analytical foundation, I am transitioning toward **ML & AI systems engineering**:  
- Integrating ML models into production-ready pipelines (with MLflow / SageMaker)  
- Exploring **Transformer architectures, LLM-based pipelines**, and retrieval-augmented generation (RAG)  
- Bridging **data engineering, machine learning, and AI applications** end-to-end

---

## 🧠 Current Goals
- Develop a dbt-based transformation layer with modular, testable data models  
- Implement MLflow for tracking, deployment, and monitoring  
- Build an AI-ready data foundation for advanced analytics and intelligent automation

---

## 🧩 Tech Stack Overview

**Languages:** Python, SQL  
**Data Engineering:** PySpark, Airflow, Docker, dbt, AWS (S3, Glue, Athena, Redshift), Databricks  
**Streaming:** Kinesis (Kafka-based architecture)  
**Data Science:** pandas, scikit-learn, statsmodels  
**Visualization:** Tableau, QuickSight  
**Next Steps:** MLflow, SageMaker, Transformers, LangChain

---

> Building data systems that scale from pipelines to intelligence.
